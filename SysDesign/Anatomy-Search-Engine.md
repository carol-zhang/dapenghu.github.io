# The Anatomy of a Large-Scale Hypertextual Web Search Engine

# 1. Introdution
Human maintained list:
- Hard and expensive to update

Current Search Engine
- Quality of search result
- Easy to be mislead by advertiser
- Scalability: Google is designed to scale well to extremely large data sets

[Search Engine Watch](http://searchenginewatch.com/#)

## 1.1 Chanlledge
- crawling technology is needed to gather the web documents and keep them up to date
- Storage space must be used efficiently to store indices and, optionally, the documents themselves
- The indexing system must process hundreds of gigabytes of data efficiently
- Queries must be handled quickly, at a rate of hundreds to thousands per second.


## 1.2 Design Goals

One of the main causes of this problem is that the number of documents in the indices has been increasing by many orders of magnitude.
There is quite a bit of recent optimism that the use of more hypertextual information can help improve search and other applications.  In particular, link structure and link text provide a lot of information for making relevance judgments and quality filtering. Google makes use of both link structure and anchor text


# 2 Anatomy
<img src="SearchEngineArchitecture.gif" width="400" height="400/>


##   __
## Architecture
### Crawler
- There is a **URLserver** that sends lists of URLs to be fetched to the **crawlers**. 
- The web pages that are fetched are then sent to the **storeserver**. 
- The storeserver then compresses and stores the web pages into a **repository**. Every web page has an associated ID number called a docID which is assigned whenever a new URL is parsed out of a web page. 

### Indexer
- The indexing function is performed by the **indexer** and the **sorter**. 
- The indexer performs a number of functions. 
    - It reads the repository, uncompresses the documents, and parses them. 
    - Each document is converted into a set of word occurrences called **hits**. The hits record the **word**, **position** in document, an approximation of **font size**, and **capitalization**. 
    - The indexer distributes these hits into a set of "barrels", creating a partially sorted forward index. 
    - The indexer performs another important function. It parses out all the links in every web page and stores important information about them in an **anchors** file. This file contains enough information to determine where each link points from and to, and the text of the link.

### Parser
- The **URLresolver** reads the anchors file and converts relative URLs into absolute URLs and in turn into docIDs. 
- It puts the anchor text into the **forward index**. 
- It also generates a database of **links** which are pairs of docIDs. The links database is used to compute **PageRanks** for all the documents.

### Sorter
- The sorter takes the barrels, which are sorted by docID. 
- resorts them by wordID to generate the inverted index. 
- This is done in place so that little temporary space is needed for this operation. 
- The sorter also produces a list of wordIDs and offsets into the inverted index. 

### Search
- The searcher is run by a web server and uses the **lexicon** together with the **inverted index** and the **PageRanks** to answer queries.


# Major Data Structures
Google's data structures are optimized so that a large document collection can be crawled, indexed, and searched with little IO cost. 
Although, CPUs and bulk input output rates have improved dramatically over the years,** a disk seek still requires about 10 ms to complete**. 
Google is designed to avoid disk seeks whenever possible, and this has had a considerable influence on the design of the data structures.


# Sentence

- there have already been several papers using databases generated by Google, and **many others are underway**.
- "PageRank", an objective measure of its citation importance that corresponds well with people's subjective idea of importance
- The web is a vast collection of completely uncontrolled heterogeneous documents.
- 


